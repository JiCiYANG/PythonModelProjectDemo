{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f9e6e1-603c-4ac1-8de5-32061c84c931",
   "metadata": {},
   "source": [
    "### 损失函数的定义\n",
    "损失函数是用来衡量模型预测值与真实值之间差异的函数。它量化了模型预测的准确性，值越小表示模型预测越准确。\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "这是一个专门用于分类任务的损失函数，特别适用于多类别分类问题（如FashionMNIST有10个类别）。\n",
    "损失函数的作用\n",
    "评估模型性能：通过计算预测值与真实值之间的差距来评估模型的好坏\n",
    "指导模型训练：损失函数的梯度用于反向传播，指导模型参数的更新方向\n",
    "优化目标：训练过程的目标就是最小化损失函数值"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. 常见损失函数类型\n",
    "#### 回归任务\n",
    "- 均方误差 (MSE)：loss = mean((y_pred - y_true)²)\n",
    "    - 对异常值敏感，适用于噪声较小的数据\n",
    "- 平均绝对误差 (MAE)：loss = mean(|y_pred - y_true|)\n",
    "    - 对异常值鲁棒性更强\n",
    "- Huber Loss：结合MSE和MAE优点，在误差小时使用平方，在误差大时使用线性\n",
    "#### 分类任务\n",
    "- 交叉熵损失 (Cross-Entropy)：\n",
    "    - 二分类：loss = -[y*log(p) + (1-y)*log(1-p)]\n",
    "    - 多分类：loss = -sum(y_i * log(p_i))\n",
    "- 合页损失 (Hinge Loss)：主要用于SVM等模型\n",
    "- 焦点损失 (Focal Loss)：解决类别不平衡问题"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d733e3a218b3f2a8"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 回归任务\n",
    "mse_loss = nn.MSELoss()\n",
    "mae_loss = nn.L1Loss()\n",
    "\n",
    "# 分类任务\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "binary_cross_entropy = nn.BCELoss()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-27T09:50:07.571140Z",
     "start_time": "2025-07-27T09:50:06.759055Z"
    }
   },
   "id": "d89b16a5f1c653d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 计算损失值"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e496f309471971c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 前向传播得到预测值\n",
    "predictions = model(inputs)\n",
    "\n",
    "# 计算损失\n",
    "loss = loss_function(predictions, targets)\n",
    "\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77b0e5fb2a900c0a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 4. 实际应用要点\n",
    "损失函数选择原则\n",
    "根据任务类型选择（回归/分类）\n",
    "考虑数据分布特点（是否包含异常值）\n",
    "结合业务需求（不同错误类型的代价）\n",
    "训练监控\n",
    "观察训练损失和验证损失的变化趋势\n",
    "检查是否出现过拟合（训练损失持续下降但验证损失上升）\n",
    "利用学习率调度器动态调整优化过程"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9c74f40b0d57a6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#自定义损失函数\n",
    "#当标准损失函数不满足需求时，可以自定义：\n",
    "def custom_loss(y_pred, y_true):\n",
    "    # 实现特定的损失计算逻辑\n",
    "    return loss_value\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2ef28eab5cd5797"
  },
  {
   "cell_type": "markdown",
   "id": "291a57d1-ffa8-425a-9fb8-79e91e554197",
   "metadata": {},
   "source": [
    "### 工作流程\n",
    "在训练过程中：\n",
    "模型对输入数据进行预测\n",
    "使用损失函数计算预测值与真实标签之间的误差\n",
    "通过反向传播计算损失函数相对于模型参数的梯度\n",
    "优化器根据梯度更新模型参数，以减小损失值\n",
    "常见的损失函数类型\n",
    "CrossEntropyLoss：用于分类任务（如代码中使用的）\n",
    "MSELoss：用于回归任务，计算均方误差\n",
    "BCELoss：用于二分类任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4873de-70f4-487d-b985-e159b7273615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数和优化器\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# 使用随机梯度下降（SGD）作为优化算法\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)# 设置学习率为0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b74eb4-2276-4223-94ac-b72425f3e12e",
   "metadata": {},
   "source": [
    "#### SGD（Stochastic Gradient Descent，随机梯度下降）是深度学习中最基础且广泛使用的优化算法之一\n",
    "1. 基本概念\n",
    "SGD是梯度下降算法的随机变种，其核心思想是：\n",
    "梯度下降：沿着目标函数梯度的反方向更新参数以最小化损失\n",
    "随机性：每次使用一个样本或一小批样本来估计梯度，而非全部数据\n",
    "\n",
    "2. 数学原理\n",
    "参数更新公式\n",
    "θ = θ - η × ∇J(θ; x_i; y_i)\n",
    "其中：\n",
    "θ 是模型参数\n",
    "η 是学习率（learning rate）\n",
    "∇J(θ; x_i; y_i) 是损失函数对参数的梯度\n",
    "3. 与标准梯度下降的区别\n",
    "批量梯度下降：使用全部训练数据计算梯度｜稳定，收敛到全局最优｜计算开销大，速度慢。\n",
    "SGD：每次使用一个样本更新参数｜计算快，可在线学习｜更新不稳定，可能不收敛\n",
    "小批量SGD：使用小批量数据更新参数｜平衡了前两者优缺点｜需要调整批量大小"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 梯度下降的三种变体\n",
    "1. 样本计算梯度\n",
    "优点：稳定，能收敛到全局最优\n",
    "缺点：计算开销大，速度慢，不适合大数据集\n",
    "2. 随机梯度下降（SGD）\n",
    "每次使用单个样本更新参数\n",
    "优点：计算速度快，可以在线学习\n",
    "缺点：更新不稳定，可能不收敛\n",
    "3. 小批量梯度下降（Mini-batch SGD）\n",
    "使用小批量样本更新参数\n",
    "平衡了前两者的优缺点\n",
    "是目前深度学习中最常用的优化方法"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96b7009d672bc1c1"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2328f7c1-b883-4066-9628-aa81a697c05d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:56:59.818357Z",
     "start_time": "2025-07-27T09:56:59.741576Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01moptim\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01moptim\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# 使用SGD优化器\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m optimizer = optim.SGD(\u001B[43mmodel\u001B[49m.parameters(), lr=\u001B[32m1e-3\u001B[39m)\n\u001B[32m      6\u001B[39m \u001B[38;5;66;03m# 其他常用优化器\u001B[39;00m\n\u001B[32m      7\u001B[39m optimizer = optim.Adam(model.parameters(), lr=\u001B[32m1e-3\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# 定义优化器\n",
    "import torch.optim as optim\n",
    "\n",
    "# 使用SGD优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 其他常用优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde3b1a3-f431-4555-b6fb-53d22aaa1e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练循环中的使用\n",
    "# 前向传播\n",
    "predictions = model(inputs)\n",
    "loss = loss_fn(predictions, targets)\n",
    "\n",
    "# 反向传播\n",
    "optimizer.zero_grad()  # 清零梯度\n",
    "loss.backward()        # 计算梯度\n",
    "optimizer.step()       # 更新参数"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 关键参数调优\n",
    "** 学习率（Learning Rate）**\n",
    "- 过大：可能导致震荡，无法收敛\n",
    "- 过小：收敛速度慢，可能陷入局部最优\n",
    "- 常用策略：学习率衰减、自适应学习率\n",
    "** 批量大小（Batch Size）**\n",
    "影响内存使用和训练稳定性\n",
    "较小：更新频率高，但噪声较大\n",
    "较大：梯度估计更准确，但需要更多内存\n",
    "#### 实际应用要点\n",
    "训练流程\n",
    "初始化模型参数\n",
    "前向传播计算预测值\n",
    "计算损失函数\n",
    "反向传播计算梯度\n",
    "使用优化器更新参数\n",
    "重复上述过程直到收敛\n",
    "监控与调试\n",
    "观察损失函数变化趋势\n",
    "检查是否出现梯度消失或爆炸\n",
    "调整学习率和其他超参数\n",
    "使用学习率调度器动态调整"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d21d85d44abc37"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-env)",
   "language": "python",
   "name": "llm-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
