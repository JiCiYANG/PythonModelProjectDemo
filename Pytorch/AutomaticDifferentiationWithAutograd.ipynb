{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 自动求导Automatic Differentiation with torch.autograd\n",
    "在训练神经网络时，最常用的算法是 反向传播。在该算法中，根据损失函数相对于给定参数的梯度来调整参数（模型权重） 。\n",
    "\n",
    "为了计算这些梯度，PyTorch 有一个名为的内置微分引擎torch.autograd。它支持自动计算任何计算图的梯度。\n",
    "\n",
    "考虑最简单的单层神经网络，其输入为x，参数为w和b，以及一些损失函数。它可以在 PyTorch 中按以下方式定义："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "138dd975bbc0dc88"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True) # 权重参数 - 需要计算梯度\n",
    "b = torch.randn(3, requires_grad=True) # 偏置参数 - 需要计算梯度\n",
    "# 前向传播 - 构建计算图\n",
    "z = torch.matmul(x, w)+b # 计算图节点1\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y) # 计算图节点2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-29T15:05:52.768289Z",
     "start_time": "2025-07-29T15:05:51.952011Z"
    }
   },
   "id": "b837ce4cd41dfa9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### requires_grad=True 详解\n",
    "requires_grad=True 是 PyTorch 中一个非常重要的参数属性，用于控制张量是否需要计算和跟踪梯度。\n",
    "#### 基本概念\n",
    "requires_grad 是 PyTorch 张量的一个布尔属性，决定该张量是否需要参与自动微分计算：\n",
    "requires_grad=True：张量需要计算梯度，会跟踪所有在其上的操作\n",
    "requires_grad=False：张量不需要计算梯度，不会跟踪操作（默认值）\n",
    "#### 为什么需要设置 requires_grad=True？\n",
    "参数优化：神经网络的权重(w)和偏置(b)是需要在训练过程中优化的参数\n",
    "梯度计算：只有设置了 requires_grad=True 的张量，PyTorch 才会为其计算梯度\n",
    "反向传播：在反向传播过程中，只有这些张量的梯度会被计算和更新\n",
    "#### 当 requires_grad=True 时：\n",
    "PyTorch 会构建计算图，跟踪所有对张量的操作\n",
    "在调用 .backward() 时，会自动计算梯度\n",
    "梯度存储在张量的 .grad 属性中\n",
    "\n",
    "#### 相关属性和方法\n",
    ".grad：存储计算得到的梯度\n",
    ".grad_fn：指向创建该张量的函数（计算图节点）\n",
    ".backward()：执行反向传播计算梯度\n",
    "torch.no_grad()：临时禁用梯度计算"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4db2422b47442325"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing Gradients"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f51aad3180487003"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1645, 0.0804, 0.0124],\n",
      "        [0.1645, 0.0804, 0.0124],\n",
      "        [0.1645, 0.0804, 0.0124],\n",
      "        [0.1645, 0.0804, 0.0124],\n",
      "        [0.1645, 0.0804, 0.0124]])\n",
      "tensor([0.1645, 0.0804, 0.0124])\n"
     ]
    }
   ],
   "source": [
    "# 反向传播 - 自动求导\n",
    "loss.backward()\n",
    "print(w.grad)# 权重的梯度\n",
    "print(b.grad) # 偏置的梯度"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-29T15:11:36.276950Z",
     "start_time": "2025-07-29T15:11:36.248867Z"
    }
   },
   "id": "b6b13552662b46e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们只能获取grad计算图的叶节点的属性，这些节点的requires_grad属性设置为True。对于图中的所有其他节点，梯度将不可用。\n",
    "\n",
    "出于性能考虑，我们只能 backward在给定的图上执行一次梯度计算。如果我们需要backward在同一张图上执行多次调用，则需要将 传递 retain_graph=True给backward调用函数。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "532960ce61999294"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 自动求导的核心组件\n",
    "1. 计算图（Computation Graph）\n",
    "》每个操作都会创建一个计算图节点，记录操作类型和依赖关系。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ff37b10a8e4ad13"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z 的梯度函数: <AddBackward0 object at 0x10e547d30>\n",
      "loss 的梯度函数: <BinaryCrossEntropyWithLogitsBackward0 object at 0x10e5655a0>\n"
     ]
    }
   ],
   "source": [
    "print(f\"z 的梯度函数: {z.grad_fn}\")        # <AddBackward0 object>\n",
    "print(f\"loss 的梯度函数: {loss.grad_fn}\")  # <BinaryCrossEntropyWithLogitsBackward0 object>"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-07-29T15:20:19.626014Z"
    }
   },
   "id": "c747bd455ea1a0f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. 叶节点（Leaf Nodes）\n",
    "》有叶节点（如模型参数）才能获得 .grad 属性"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "796df4000b79b370"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w 是叶节点: True\n",
      "w 需要梯度: True\n",
      "z 是叶节点: False\n",
      "z 需要梯度: True\n"
     ]
    }
   ],
   "source": [
    "# 叶节点是可以计算梯度的张量\n",
    "print(f\"w 是叶节点: {w.is_leaf}\")      # True\n",
    "print(f\"w 需要梯度: {w.requires_grad}\") # True\n",
    "\n",
    "# 中间节点通常不是叶节点\n",
    "print(f\"z 是叶节点: {z.is_leaf}\")      # False\n",
    "print(f\"z 需要梯度: {z.requires_grad}\") # True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-29T15:21:10.036106Z",
     "start_time": "2025-07-29T15:21:10.033209Z"
    }
   },
   "id": "75e6b4ae7e0e51e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. 梯度计算"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "165d8a5c65748fff"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# 执行反向传播\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# 梯度存储在 .grad 属性中\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m权重梯度:\u001B[39m\u001B[33m\"\u001B[39m, w.grad.shape)  \u001B[38;5;66;03m# torch.Size([5, 3])\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:648\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    638\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    639\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    640\u001B[39m         Tensor.backward,\n\u001B[32m    641\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    646\u001B[39m         inputs=inputs,\n\u001B[32m    647\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m648\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    650\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    348\u001B[39m     retain_graph = create_graph\n\u001B[32m    350\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m353\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    822\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    823\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m824\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    825\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    826\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    827\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    828\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mRuntimeError\u001B[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# 执行反向传播\n",
    "loss.backward()\n",
    "\n",
    "# 梯度存储在 .grad 属性中\n",
    "print(\"权重梯度:\", w.grad.shape)  # torch.Size([5, 3])\n",
    "print(\"偏置梯度:\", b.grad.shape)  # torch.Size([3])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-29T15:22:08.901780Z",
     "start_time": "2025-07-29T15:22:08.765428Z"
    }
   },
   "id": "e5f0228a8158b268"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Disabling Gradient Tracking\n",
    "默认情况下，所有带有requires_grad=True的张量都会跟踪其计算历史并支持梯度计算。然而，在某些情况下我们不需要这样做，例如，当我们已经训练好模型，只想将其应用于一些输入数据时，即我们只想通过网络 进行前向torch.no_grad()计算。我们可以通过在计算代码周围添加以下代码块来停止跟踪计算："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d737bcb9610939aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fba0cd0731e9aad6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "实现相同结果的另一种方法是使用detach()张量上的方法："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14aa67014ef1c540"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed033b4cb0096f73"
  },
  {
   "cell_type": "markdown",
   "source": [
    "可能希望禁用梯度跟踪的原因如下：\n",
    "将神经网络中的某些参数标记为冻结参数。\n",
    "\n",
    "当您仅进行前向传递时，可以加快计算速度，因为不跟踪梯度的张量的计算会更有效率。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2036a885477a083"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 自动求导的两种模式\n",
    "1. 训练模式（需要梯度）"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78c7eda0412bab66"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 默认模式 - 跟踪计算历史\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)  # True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb6010f6540e4990"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. 推理模式（不需要梯度）"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15fd1299cf0ad359"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 方法1: 使用 torch.no_grad() 上下文管理器\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "    print(z.requires_grad)  # False\n",
    "\n",
    "# 方法2: 使用 detach() 方法\n",
    "z = torch.matmul(x, w) + b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)  # False\n",
    "\n",
    "# 方法3: 禁用全局梯度计算\n",
    "torch.set_grad_enabled(False)\n",
    "z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)  # False\n",
    "\n",
    "torch.set_grad_enabled(True)  # 恢复"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa4f9dfe447bdc50"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weight 的梯度形状: torch.Size([3, 5])\n",
      "linear.bias 的梯度形状: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义简单神经网络\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(5, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# 创建模型和数据\n",
    "model = SimpleNet()\n",
    "x = torch.randn(2, 5)\n",
    "y_true = torch.randn(2, 3)\n",
    "\n",
    "# 前向传播\n",
    "y_pred = model(x)\n",
    "loss = nn.MSELoss()(y_pred, y_true)\n",
    "\n",
    "# 自动求导\n",
    "loss.backward()\n",
    "\n",
    "# 查看参数梯度\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name} 的梯度形状: {param.grad.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-29T15:33:52.432338Z",
     "start_time": "2025-07-29T15:33:52.427250Z"
    }
   },
   "id": "24fcac764c79050c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
